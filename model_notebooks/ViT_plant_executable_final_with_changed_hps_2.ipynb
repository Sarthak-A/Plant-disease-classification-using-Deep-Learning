{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6b8hv6CWyVd",
        "outputId": "249fb0e8-9c7d-483c-cb65-10c4f364bd34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: patchify in c:\\users\\ankit\\anaconda3\\envs\\plantdisease\\lib\\site-packages (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ankit\\anaconda3\\envs\\plantdisease\\lib\\site-packages (from patchify) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install patchify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dcG9OvczHUE",
        "outputId": "b7a30f00-15df-400f-9f16-32a50410354c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 256, 768)]   0           []                               \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 256, 768)     590592      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 256, 768)    0           ['dense_8[0][0]']                \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " class_token_1 (ClassToken)     (None, 1, 768)       768         ['tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 257, 768)     0           ['class_token_1[0][0]',          \n",
            "                                                                  'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 257, 768)    1536        ['concatenate_1[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 257, 768)    28339968    ['layer_normalization_7[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 257, 768)     0           ['multi_head_attention_3[0][0]', \n",
            "                                                                  'concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 257, 768)    1536        ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 257, 3072)    2362368     ['layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 257, 3072)    0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 257, 768)     2360064     ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 257, 768)     0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 257, 768)     0           ['dropout_7[0][0]',              \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 257, 768)    1536        ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 257, 768)    28339968    ['layer_normalization_9[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 257, 768)     0           ['multi_head_attention_4[0][0]', \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 257, 768)    1536        ['add_8[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 257, 3072)    2362368     ['layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 257, 3072)    0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 257, 768)     2360064     ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 257, 768)     0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 257, 768)     0           ['dropout_9[0][0]',              \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 257, 768)    1536        ['add_9[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 257, 768)    28339968    ['layer_normalization_11[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 257, 768)     0           ['multi_head_attention_5[0][0]', \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 257, 768)    1536        ['add_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 257, 3072)    2362368     ['layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 257, 3072)    0           ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 257, 768)     2360064     ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 257, 768)     0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 257, 768)     0           ['dropout_11[0][0]',             \n",
            "                                                                  'add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 257, 768)    1536        ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['layer_normalization_13[0][0]'] \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 88)           67672       ['tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 99,856,984\n",
            "Trainable params: 99,856,984\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class ClassToken(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable = True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls\n",
        "\n",
        "def mlp(x, cf):\n",
        "    x = Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    x = Dense(cf[\"hidden_dim\"])(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    return x\n",
        "\n",
        "def transformer_encoder(x, cf):\n",
        "    skip_1 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = MultiHeadAttention(\n",
        "        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n",
        "    )(x, x)\n",
        "    x = Add()([x, skip_1])\n",
        "\n",
        "    skip_2 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = mlp(x, cf)\n",
        "    x = Add()([x, skip_2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def ViT(cf):\n",
        "    \"\"\" Inputs \"\"\"\n",
        "    input_shape = (cf[\"num_patches\"], cf[\"patch_size\"]*cf[\"patch_size\"]*cf[\"num_channels\"])\n",
        "    inputs = Input(input_shape)     ## (None, 256, 3072)\n",
        "\n",
        "    \"\"\" Patch + Position Embeddings \"\"\"\n",
        "    patch_embed = Dense(cf[\"hidden_dim\"])(inputs)   ## (None, 256, 768)\n",
        "\n",
        "    positions = tf.range(start=0, limit=cf[\"num_patches\"], delta=1)\n",
        "    pos_embed = Embedding(input_dim=cf[\"num_patches\"], output_dim=cf[\"hidden_dim\"])(positions) ## (256, 768)\n",
        "    embed = patch_embed + pos_embed ## (None, 256, 768)\n",
        "\n",
        "    \"\"\" Adding Class Token \"\"\"\n",
        "    token = ClassToken()(embed)\n",
        "    x = Concatenate(axis=1)([token, embed]) ## (None, 257, 768)\n",
        "\n",
        "    for _ in range(cf[\"num_layers\"]):\n",
        "        x = transformer_encoder(x, cf)\n",
        "\n",
        "    \"\"\" Classification Head \"\"\"\n",
        "    x = LayerNormalization()(x)     ## (None, 257, 768)\n",
        "    x = x[:, 0, :]\n",
        "    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {}\n",
        "    config[\"num_layers\"] = 3\n",
        "    config[\"hidden_dim\"] = 768\n",
        "    config[\"mlp_dim\"] = 3072\n",
        "    config[\"num_heads\"] = 12\n",
        "    config[\"dropout_rate\"] = 0.5 #from 0.1 to 0.5\n",
        "    config[\"num_patches\"] = 256 #from 64 to 256\n",
        "    config[\"patch_size\"] = 16 #from 32 to 16\n",
        "    config[\"num_channels\"] = 3\n",
        "    config[\"num_classes\"] = 88\n",
        "    model = ViT(config)\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DaOmFtHwpEu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle   #Need to disable real time scanning of McAfee software for this to run\n",
        "# import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from patchify import patchify\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
        "#from vit import ViT\n",
        "\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "hp = {}\n",
        "hp[\"image_size\"] = 256\n",
        "hp[\"num_channels\"] = 3\n",
        "hp[\"patch_size\"] = 16 #from 32 to 16\n",
        "hp[\"num_patches\"] = (hp[\"image_size\"]**2) // (hp[\"patch_size\"]**2)\n",
        "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"]*hp[\"patch_size\"]*hp[\"num_channels\"])\n",
        "\n",
        "hp[\"batch_size\"] = 16 #from 64 to 16\n",
        "hp[\"lr\"] = 2e-5 #from 1e-4\n",
        "hp[\"num_epochs\"] = 45\n",
        "hp[\"num_classes\"] = 88\n",
        "hp[\"class_names\"] = ['Apple__black_rot',\n",
        "                     'Apple__healthy',\n",
        "                     'Apple__rust',\n",
        "                     'Apple__scab',\n",
        "                     'Cassava__bacterial_blight',\n",
        "                     'Cassava__brown_streak_disease',\n",
        "                     'Cassava__green_mottle',\n",
        "                     'Cassava__healthy',\n",
        "                     'Cassava__mosaic_disease',\n",
        "                     'Cherry__healthy',\n",
        "                     'Cherry__powdery_mildew',\n",
        "                     'Chili__healthy',\n",
        "                     'Chili__leaf curl',\n",
        "                     'Chili__leaf spot',\n",
        "                     'Chili__whitefly',\n",
        "                     'Chili__yellowish',\n",
        "                     'Coffee__cercospora_leaf_spot',\n",
        "                     'Coffee__healthy',\n",
        "                     'Coffee__red_spider_mite',\n",
        "                     'Coffee__rust',\n",
        "                     'Corn__common_rust',\n",
        "                     'Corn__gray_leaf_spot',\n",
        "                     'Corn__healthy',\n",
        "                     'Corn__northern_leaf_blight',\n",
        "                     'Cucumber__diseased',\n",
        "                     'Cucumber__healthy',\n",
        "                     'Gauva__diseased',\n",
        "                     'Gauva__healthy',\n",
        "                     'Grape__black_measles',\n",
        "                     'Grape__black_rot',\n",
        "                     'Grape__healthy',\n",
        "                     'Grape__leaf_blight_(isariopsis_leaf_spot)',\n",
        "                     'Jamun__diseased',\n",
        "                     'Jamun__healthy',\n",
        "                     'Lemon__diseased',\n",
        "                     'Lemon__healthy',\n",
        "                     'Mango__diseased',\n",
        "                     'Mango__healthy',\n",
        "                     'Peach__bacterial_spot',\n",
        "                     'Peach__healthy',\n",
        "                     'Pepper_bell__bacterial_spot',\n",
        "                     'Pepper_bell__healthy',\n",
        "                     'Pomegranate__diseased',\n",
        "                     'Pomegranate__healthy',\n",
        "                     'Potato__early_blight',\n",
        "                     'Potato__healthy',\n",
        "                     'Potato__late_blight',\n",
        "                     'Rice__brown_spot',\n",
        "                     'Rice__healthy',\n",
        "                     'Rice__hispa',\n",
        "                     'Rice__leaf_blast',\n",
        "                     'Rice__neck_blast',\n",
        "                     'Soybean__bacterial_blight',\n",
        "                     'Soybean__caterpillar',\n",
        "                     'Soybean__diabrotica_speciosa',\n",
        "                     'Soybean__downy_mildew',\n",
        "                     'Soybean__healthy',\n",
        "                     'Soybean__mosaic_virus',\n",
        "                     'Soybean__powdery_mildew',\n",
        "                     'Soybean__rust',\n",
        "                     'Soybean__southern_blight',\n",
        "                     'Strawberry___leaf_scorch',\n",
        "                     'Strawberry__healthy',\n",
        "                     'Sugarcane__bacterial_blight',\n",
        "                     'Sugarcane__healthy',\n",
        "                     'Sugarcane__red_rot',\n",
        "                     'Sugarcane__red_stripe',\n",
        "                     'Sugarcane__rust',\n",
        "                     'Tea__algal_leaf',\n",
        "                     'Tea__anthracnose',\n",
        "                     'Tea__bird_eye_spot',\n",
        "                     'Tea__brown_blight',\n",
        "                     'Tea__healthy',\n",
        "                     'Tea__red_leaf_spot',\n",
        "                     'Tomato__bacterial_spot',\n",
        "                     'Tomato__early_blight',\n",
        "                     'Tomato__healthy',\n",
        "                     'Tomato__late_blight',\n",
        "                     'Tomato__leaf_mold',\n",
        "                     'Tomato__mosaic_virus',\n",
        "                     'Tomato__septoria_leaf_spot',\n",
        "                     'Tomato__spider_mites_(two_spotted_spider_mite)',\n",
        "                     'Tomato__target_spot',\n",
        "                     'Tomato__yellow_leaf_curl_virus',\n",
        "                     'Wheat__brown_rust',\n",
        "                     'Wheat__healthy',\n",
        "                     'Wheat__septoria',\n",
        "                     'Wheat__yellow_rust']\n",
        "hp[\"num_layers\"] = 3\n",
        "hp[\"hidden_dim\"] = 768\n",
        "hp[\"mlp_dim\"] = 3072\n",
        "hp[\"num_heads\"] = 12\n",
        "hp[\"dropout_rate\"] = 0.5 #from 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUfFzOJ-R5yE",
        "outputId": "b9958d8d-e216-40b9-b70e-ff2c17eb5424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of images: 79083\n",
            "Sample images: ['C:\\\\Users\\\\ankit\\\\PDMD\\\\Pomegranate__diseased\\\\Pomegranate__diseased_111.JPG', 'C:\\\\Users\\\\ankit\\\\PDMD\\\\Peach__bacterial_spot\\\\Peach__bacterial_spot_1355.JPG', 'C:\\\\Users\\\\ankit\\\\PDMD\\\\Soybean__healthy\\\\Soybean__healthy_4686.jpg', 'C:\\\\Users\\\\ankit\\\\PDMD\\\\Tomato__late_blight\\\\Tomato__late_blight_31.JPG', 'C:\\\\Users\\\\ankit\\\\PDMD\\\\Grape__leaf_blight_(isariopsis_leaf_spot)\\\\Grape__leaf_blight_(isariopsis_leaf_spot)_343.JPG']\n",
            "Split size: 23724\n",
            "Train: 55359 - Valid: 15895 - Test: 7829\n",
            "Epoch 1/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 1.9259 - acc: 0.4946\n",
            "Epoch 1: val_loss improved from inf to 1.07465, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 976s 281ms/step - loss: 1.9259 - acc: 0.4946 - val_loss: 1.0746 - val_acc: 0.6830 - lr: 2.0000e-05\n",
            "Epoch 2/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.9121 - acc: 0.7253\n",
            "Epoch 2: val_loss improved from 1.07465 to 0.77875, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 957s 277ms/step - loss: 0.9121 - acc: 0.7253 - val_loss: 0.7788 - val_acc: 0.7602 - lr: 2.0000e-05\n",
            "Epoch 3/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.6940 - acc: 0.7848\n",
            "Epoch 3: val_loss improved from 0.77875 to 0.65184, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 961s 278ms/step - loss: 0.6940 - acc: 0.7848 - val_loss: 0.6518 - val_acc: 0.7973 - lr: 2.0000e-05\n",
            "Epoch 4/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.5827 - acc: 0.8152\n",
            "Epoch 4: val_loss improved from 0.65184 to 0.61453, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 1043s 301ms/step - loss: 0.5827 - acc: 0.8152 - val_loss: 0.6145 - val_acc: 0.8055 - lr: 2.0000e-05\n",
            "Epoch 5/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.5053 - acc: 0.8390\n",
            "Epoch 5: val_loss improved from 0.61453 to 0.54668, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 1192s 344ms/step - loss: 0.5053 - acc: 0.8390 - val_loss: 0.5467 - val_acc: 0.8281 - lr: 2.0000e-05\n",
            "Epoch 6/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.4521 - acc: 0.8528\n",
            "Epoch 6: val_loss improved from 0.54668 to 0.52933, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 1124s 325ms/step - loss: 0.4521 - acc: 0.8528 - val_loss: 0.5293 - val_acc: 0.8336 - lr: 2.0000e-05\n",
            "Epoch 7/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.4070 - acc: 0.8665\n",
            "Epoch 7: val_loss did not improve from 0.52933\n",
            "3460/3460 [==============================] - 997s 288ms/step - loss: 0.4070 - acc: 0.8665 - val_loss: 0.5356 - val_acc: 0.8311 - lr: 2.0000e-05\n",
            "Epoch 8/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.3692 - acc: 0.8779\n",
            "Epoch 8: val_loss improved from 0.52933 to 0.52093, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 929s 268ms/step - loss: 0.3692 - acc: 0.8779 - val_loss: 0.5209 - val_acc: 0.8374 - lr: 2.0000e-05\n",
            "Epoch 9/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.3414 - acc: 0.8864\n",
            "Epoch 9: val_loss improved from 0.52093 to 0.50091, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 925s 267ms/step - loss: 0.3414 - acc: 0.8864 - val_loss: 0.5009 - val_acc: 0.8454 - lr: 2.0000e-05\n",
            "Epoch 10/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.3089 - acc: 0.8962\n",
            "Epoch 10: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 918s 265ms/step - loss: 0.3089 - acc: 0.8962 - val_loss: 0.5066 - val_acc: 0.8443 - lr: 2.0000e-05\n",
            "Epoch 11/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.2872 - acc: 0.9026\n",
            "Epoch 11: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 916s 265ms/step - loss: 0.2872 - acc: 0.9026 - val_loss: 0.5268 - val_acc: 0.8456 - lr: 2.0000e-05\n",
            "Epoch 12/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.2659 - acc: 0.9088\n",
            "Epoch 12: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 915s 264ms/step - loss: 0.2659 - acc: 0.9088 - val_loss: 0.5303 - val_acc: 0.8433 - lr: 2.0000e-05\n",
            "Epoch 13/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.2448 - acc: 0.9140\n",
            "Epoch 13: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 920s 266ms/step - loss: 0.2448 - acc: 0.9140 - val_loss: 0.5435 - val_acc: 0.8439 - lr: 2.0000e-05\n",
            "Epoch 14/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.2266 - acc: 0.9206\n",
            "Epoch 14: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 929s 268ms/step - loss: 0.2266 - acc: 0.9206 - val_loss: 0.5407 - val_acc: 0.8498 - lr: 2.0000e-05\n",
            "Epoch 15/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.2081 - acc: 0.9264\n",
            "Epoch 15: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 930s 269ms/step - loss: 0.2081 - acc: 0.9264 - val_loss: 0.5886 - val_acc: 0.8376 - lr: 2.0000e-05\n",
            "Epoch 16/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.1930 - acc: 0.9320\n",
            "Epoch 16: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 934s 270ms/step - loss: 0.1930 - acc: 0.9320 - val_loss: 0.5392 - val_acc: 0.8542 - lr: 2.0000e-05\n",
            "Epoch 17/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.1763 - acc: 0.9381\n",
            "Epoch 17: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 939s 272ms/step - loss: 0.1763 - acc: 0.9381 - val_loss: 0.5808 - val_acc: 0.8488 - lr: 2.0000e-05\n",
            "Epoch 18/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.1647 - acc: 0.9411\n",
            "Epoch 18: val_loss did not improve from 0.50091\n",
            "3460/3460 [==============================] - 939s 271ms/step - loss: 0.1647 - acc: 0.9411 - val_loss: 0.5731 - val_acc: 0.8483 - lr: 2.0000e-05\n",
            "Epoch 19/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.1527 - acc: 0.9459\n",
            "Epoch 19: val_loss did not improve from 0.50091\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
            "3460/3460 [==============================] - 937s 271ms/step - loss: 0.1527 - acc: 0.9459 - val_loss: 0.5572 - val_acc: 0.8531 - lr: 2.0000e-05\n",
            "Epoch 20/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0895 - acc: 0.9697\n",
            "Epoch 20: val_loss improved from 0.50091 to 0.44581, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 938s 271ms/step - loss: 0.0895 - acc: 0.9697 - val_loss: 0.4458 - val_acc: 0.8845 - lr: 2.0000e-06\n",
            "Epoch 21/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0710 - acc: 0.9767\n",
            "Epoch 21: val_loss improved from 0.44581 to 0.44421, saving model to files\\model_pdmd_final.h5\n",
            "3460/3460 [==============================] - 951s 275ms/step - loss: 0.0710 - acc: 0.9767 - val_loss: 0.4442 - val_acc: 0.8854 - lr: 2.0000e-06\n",
            "Epoch 22/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0616 - acc: 0.9811\n",
            "Epoch 22: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 946s 273ms/step - loss: 0.0616 - acc: 0.9811 - val_loss: 0.4550 - val_acc: 0.8849 - lr: 2.0000e-06\n",
            "Epoch 23/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0556 - acc: 0.9836\n",
            "Epoch 23: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 924s 267ms/step - loss: 0.0556 - acc: 0.9836 - val_loss: 0.4573 - val_acc: 0.8852 - lr: 2.0000e-06\n",
            "Epoch 24/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0511 - acc: 0.9847\n",
            "Epoch 24: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 925s 267ms/step - loss: 0.0511 - acc: 0.9847 - val_loss: 0.4705 - val_acc: 0.8839 - lr: 2.0000e-06\n",
            "Epoch 25/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0468 - acc: 0.9863\n",
            "Epoch 25: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 920s 266ms/step - loss: 0.0468 - acc: 0.9863 - val_loss: 0.4835 - val_acc: 0.8830 - lr: 2.0000e-06\n",
            "Epoch 26/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0426 - acc: 0.9880\n",
            "Epoch 26: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 918s 265ms/step - loss: 0.0426 - acc: 0.9880 - val_loss: 0.4896 - val_acc: 0.8831 - lr: 2.0000e-06\n",
            "Epoch 27/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0408 - acc: 0.9886\n",
            "Epoch 27: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 915s 264ms/step - loss: 0.0408 - acc: 0.9886 - val_loss: 0.4988 - val_acc: 0.8832 - lr: 2.0000e-06\n",
            "Epoch 28/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0368 - acc: 0.9898\n",
            "Epoch 28: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 919s 266ms/step - loss: 0.0368 - acc: 0.9898 - val_loss: 0.5053 - val_acc: 0.8834 - lr: 2.0000e-06\n",
            "Epoch 29/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0348 - acc: 0.9900\n",
            "Epoch 29: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 921s 266ms/step - loss: 0.0348 - acc: 0.9900 - val_loss: 0.5193 - val_acc: 0.8817 - lr: 2.0000e-06\n",
            "Epoch 30/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0324 - acc: 0.9913\n",
            "Epoch 30: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 927s 268ms/step - loss: 0.0324 - acc: 0.9913 - val_loss: 0.5232 - val_acc: 0.8843 - lr: 2.0000e-06\n",
            "Epoch 31/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0304 - acc: 0.9917\n",
            "Epoch 31: val_loss did not improve from 0.44421\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.9999999949504855e-07.\n",
            "3460/3460 [==============================] - 930s 269ms/step - loss: 0.0304 - acc: 0.9917 - val_loss: 0.5351 - val_acc: 0.8820 - lr: 2.0000e-06\n",
            "Epoch 32/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0269 - acc: 0.9929\n",
            "Epoch 32: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 932s 269ms/step - loss: 0.0269 - acc: 0.9929 - val_loss: 0.5096 - val_acc: 0.8884 - lr: 2.0000e-07\n",
            "Epoch 33/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0252 - acc: 0.9937\n",
            "Epoch 33: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 943s 272ms/step - loss: 0.0252 - acc: 0.9937 - val_loss: 0.5100 - val_acc: 0.8881 - lr: 2.0000e-07\n",
            "Epoch 34/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0244 - acc: 0.9939\n",
            "Epoch 34: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 936s 271ms/step - loss: 0.0244 - acc: 0.9939 - val_loss: 0.5112 - val_acc: 0.8882 - lr: 2.0000e-07\n",
            "Epoch 35/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0239 - acc: 0.9942\n",
            "Epoch 35: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 937s 271ms/step - loss: 0.0239 - acc: 0.9942 - val_loss: 0.5120 - val_acc: 0.8875 - lr: 2.0000e-07\n",
            "Epoch 36/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0237 - acc: 0.9939\n",
            "Epoch 36: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 938s 271ms/step - loss: 0.0237 - acc: 0.9939 - val_loss: 0.5115 - val_acc: 0.8881 - lr: 2.0000e-07\n",
            "Epoch 37/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0229 - acc: 0.9945\n",
            "Epoch 37: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 936s 270ms/step - loss: 0.0229 - acc: 0.9945 - val_loss: 0.5137 - val_acc: 0.8877 - lr: 2.0000e-07\n",
            "Epoch 38/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0226 - acc: 0.9946\n",
            "Epoch 38: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 933s 270ms/step - loss: 0.0226 - acc: 0.9946 - val_loss: 0.5146 - val_acc: 0.8878 - lr: 2.0000e-07\n",
            "Epoch 39/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0227 - acc: 0.9945\n",
            "Epoch 39: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 925s 267ms/step - loss: 0.0227 - acc: 0.9945 - val_loss: 0.5165 - val_acc: 0.8880 - lr: 2.0000e-07\n",
            "Epoch 40/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0224 - acc: 0.9946\n",
            "Epoch 40: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 926s 268ms/step - loss: 0.0224 - acc: 0.9946 - val_loss: 0.5178 - val_acc: 0.8878 - lr: 2.0000e-07\n",
            "Epoch 41/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0214 - acc: 0.9950\n",
            "Epoch 41: val_loss did not improve from 0.44421\n",
            "\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 2.000000023372195e-08.\n",
            "3460/3460 [==============================] - 916s 265ms/step - loss: 0.0214 - acc: 0.9950 - val_loss: 0.5168 - val_acc: 0.8875 - lr: 2.0000e-07\n",
            "Epoch 42/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0215 - acc: 0.9949\n",
            "Epoch 42: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 918s 265ms/step - loss: 0.0215 - acc: 0.9949 - val_loss: 0.5173 - val_acc: 0.8880 - lr: 2.0000e-08\n",
            "Epoch 43/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0213 - acc: 0.9948\n",
            "Epoch 43: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 916s 265ms/step - loss: 0.0213 - acc: 0.9948 - val_loss: 0.5177 - val_acc: 0.8878 - lr: 2.0000e-08\n",
            "Epoch 44/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0212 - acc: 0.9950\n",
            "Epoch 44: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 918s 265ms/step - loss: 0.0212 - acc: 0.9950 - val_loss: 0.5180 - val_acc: 0.8878 - lr: 2.0000e-08\n",
            "Epoch 45/45\n",
            "3460/3460 [==============================] - ETA: 0s - loss: 0.0215 - acc: 0.9948\n",
            "Epoch 45: val_loss did not improve from 0.44421\n",
            "3460/3460 [==============================] - 925s 267ms/step - loss: 0.0215 - acc: 0.9948 - val_loss: 0.5182 - val_acc: 0.8880 - lr: 2.0000e-08\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# def load_data(path, split=0.1):\n",
        "#     images = shuffle(glob(os.path.join(path, \"*\", \"*.JPG\" or \".*jpg\")))\n",
        "\n",
        "#     split_size = int(len(images) * split)\n",
        "#     train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "#     train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
        "\n",
        "#     return train_x, valid_x, test_x\n",
        "\n",
        "def load_data(path, split=0.3):\n",
        "    images = shuffle(glob(os.path.join(path, \"*\", \"*.JPG\" or \".*jpg\")))\n",
        "    # random.shuffle(images)\n",
        "\n",
        "    print(\"Total number of images:\", len(images))\n",
        "    print(\"Sample images:\", images[:5])  # Print first 5 image paths\n",
        "\n",
        "    # Ensure there's at least one image for testing\n",
        "    min_test_size = 1\n",
        "    split_size = max(min_test_size, int(len(images) * split))\n",
        "    print(\"Split size:\", split_size)\n",
        "\n",
        "    train_x, valid_test_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    valid_x, test_x = train_test_split(valid_test_x, test_size=0.33, random_state=42)\n",
        "\n",
        "    return train_x, valid_x, test_x\n",
        "\n",
        "\n",
        "def process_image_label(path):\n",
        "    \"\"\" Reading images \"\"\"\n",
        "    path = path.decode()\n",
        "    #print(\"Processing image path:\", path)  # Print the path before processing\n",
        "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n",
        "    image = image/255.0\n",
        "\n",
        "    \"\"\" Preprocessing to patches \"\"\"\n",
        "    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n",
        "    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n",
        "\n",
        "    patches = np.reshape(patches, hp[\"flat_patches_shape\"])\n",
        "    patches = patches.astype(np.float32)\n",
        "\n",
        "    \"\"\" Label \"\"\"\n",
        "    class_name = path.split(\"\\\\\")[-2]  # Adjust to split using \"\\\"\n",
        "    class_idx = hp[\"class_names\"].index(class_name)\n",
        "    class_idx = np.array(class_idx, dtype=np.int32)\n",
        "\n",
        "    return patches, class_idx\n",
        "\n",
        "def parse(path):\n",
        "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
        "    labels = tf.one_hot(labels, hp[\"num_classes\"])\n",
        "\n",
        "    patches.set_shape(hp[\"flat_patches_shape\"])\n",
        "    labels.set_shape(hp[\"num_classes\"])\n",
        "\n",
        "    return patches, labels\n",
        "\n",
        "def tf_dataset(images, batch=32):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
        "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
        "    return ds\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory for storing files \"\"\"\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    \"\"\" Paths \"\"\"\n",
        "    dataset_path = r\"C:\\Users\\ankit\\PDMD\"\n",
        "    model_path = os.path.join(\"files\", \"model_pdmd_final.h5\")\n",
        "    csv_path = os.path.join(\"files\", \"log.csv\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    train_x, valid_x, test_x = load_data(dataset_path)\n",
        "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
        "\n",
        "    train_ds = tf_dataset(train_x, batch=hp[\"batch_size\"])\n",
        "    valid_ds = tf_dataset(valid_x, batch=hp[\"batch_size\"])\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model = ViT(hp)\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n",
        "        metrics=[\"acc\"]\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n",
        "    ]\n",
        "\n",
        "    hist = model.fit(\n",
        "        train_ds,\n",
        "        epochs=hp[\"num_epochs\"],\n",
        "        validation_data=valid_ds,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    ## ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0-H40XkevB8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZDViQnizHWz",
        "outputId": "ac0b97bf-832f-46c4-bdd9-7bda965cd710"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fig\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(hist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig=plt.figure()\n",
        "plt.plot(hist.history['loss'], color='blue', label='loss')\n",
        "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
        "fig.suptitle('Loss')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwH9_Qv8zHZi"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(hist.history['acc'], color ='blue', label='accuracy')\n",
        "plt.plot(hist.history['val_acc'], color ='orange', label='val_accuracy')\n",
        "fig.suptitle('Accuracy')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VdUywuGzHb5",
        "outputId": "0d2f714a-9c5e-4752-e6dc-592bb59d196a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf_dataset\u001b[49m(test_x, batch\u001b[38;5;241m=\u001b[39mhp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Predict classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tf_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load test data\n",
        "test_ds = tf_dataset(test_x, batch=hp[\"batch_size\"])\n",
        "\n",
        "# Predict classes\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "    y_pred.extend(np.argmax(model.predict(images), axis=1))\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy8Wav07zHes"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate overall test accuracy\n",
        "test_accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Overall Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuzO6vX3zHhP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnBWl8h-lgdT"
      },
      "outputs": [],
      "source": [
        "# Calculate class-wise metrics\n",
        "class_accuracy = []\n",
        "class_precision = []\n",
        "class_recall = []\n",
        "class_f1 = []\n",
        "\n",
        "for i in range(len(hp[\"class_names\"])):\n",
        "    class_y_true = [1 if x == i else 0 for x in y_true]\n",
        "    class_y_pred = [1 if x == i else 0 for x in y_pred]\n",
        "\n",
        "    class_accuracy.append(accuracy_score(class_y_true, class_y_pred))\n",
        "    class_precision.append(precision_score(class_y_true, class_y_pred))\n",
        "    class_recall.append(recall_score(class_y_true, class_y_pred))\n",
        "    class_f1.append(f1_score(class_y_true, class_y_pred))\n",
        "\n",
        "for i, class_name in enumerate(hp[\"class_names\"]):\n",
        "    print(f\"Class: {class_name}\")\n",
        "    print(f\"Accuracy: {class_accuracy[i]}\")\n",
        "    print(f\"Precision: {class_precision[i]}\")\n",
        "    print(f\"Recall: {class_recall[i]}\")\n",
        "    print(f\"F1 Score: {class_f1[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUg9MM7nlggD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNABmK9Ulgip"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPs7s5BflglA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H7QjXthlgno"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQjqlF8nlgqd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll0FkP2ylgs1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlLkaM_glgvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5oNO05RlgyB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGDnMjxdlg0p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1cYQgk-lg29"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}